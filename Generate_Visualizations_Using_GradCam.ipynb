{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BurgundyIsAPublicEnemy/ORLIAn/blob/main/Generate_Visualizations_Using_GradCam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate a visualization using GradCam\n",
        "This notebook allows you to generate visualizations for the Eln-AI-ny model\n",
        "\n",
        "Usage: \n",
        "1. Once you have a model trained, configure the bottom lines (follow variable names) \n",
        "2. Hit run\n",
        "3. Check FOLDER_TO_OUTPUT to see where your models have been outputted"
      ],
      "metadata": {
        "id": "a0RtwQJj3hFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --upgrade\n",
        "!pip install pytorch-gradcam\n",
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "oVakr-Q7cODM",
        "outputId": "449941c6-9be5-4a3a-8cc3-a3a0825f83c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colab-env in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: python-dotenv<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from colab-env) (0.19.2)\n",
            "Requirement already satisfied: pytorch-gradcam in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-gradcam) (1.19.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from pytorch-gradcam) (4.1.2.30)\n",
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.7/dist-packages (1.5.8)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.10.0.2)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.62.3)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.10.0+cu111)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.18.2)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2022.1.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.7.0)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.6.2)\n",
            "Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.3.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.43.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.10)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "\n",
        "colab_env.__version__"
      ],
      "metadata": {
        "id": "Be4dDUqpcRCT",
        "outputId": "dc4d76e7-e822-4a86-fe8d-5f519e51e438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Python-dotenv could not parse statement starting at line 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.2.0'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!more /content/gdrive/MyDrive/vars.env"
      ],
      "metadata": {
        "id": "CoD0xcexcSXi",
        "outputId": "1e7cad24-45d9-4ae7-9da7-95e309df8f17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMAGEFILES = 'WHERE YOUR DATAIKU IMAGES ARE LOCATED'\n",
            "TABULARTRAINING = 'WHERE THE TRAINNG CSV IS AT'\n",
            "TABULARTEST = 'WHERE THE TEST CSV IS AT '\n",
            "FULLSTACKTRAIN = 'WHERE YOUR TIFS ARE'\n",
            "TRAIN_AUGMENTED_PATH = 'WHERE YOU WANT TO PUT YOUR NEW TRAINING CSV'\n",
            "TEST_AUGMENTED_PATH = 'WHERE YOU WANT TO PUT YOUR NEW TEST CSV'\n",
            "METADATA_PATH = 'THE PARENT FOLDER FOR TRAIN_AUGMENTED_PATH AND TEST_AUGMENTED_P\n",
            "ATH - ENDS WITH A /\n",
            "TENPERCENT_MODEL_PATH = 'WHERE TEN PERCENT IS STORED'\n",
            "IMG_TO_ANALYZE = 'WHAT IMAGE YOU WANT TO RUN VISUALIZATIONS ON'\n",
            "FOLDER_TO_OUTPUT = 'THE FOLDER YOU WANT TO TO STORE YOUR IMAGE IN'\n",
            "MODEL_PATH = 'OUTPUT YOUR TRAINED MODELS'\n",
            "patience = 'How long you want to be patient before early stopping'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "TENPERCENT_MODEL_PATH = os.getenv(\"TENPERCENT_MODEL_PATH\")\n",
        "IMG_TO_ANALYZE = os.getenv(\"IMG_TO_ANALYZE\")\n",
        "FOLDER_TO_OUTPUT = os.getenv(\"FOLDER_TO_OUTPUT\")\n",
        "MODEL_PATH = os.getenv(\"MODEL_PATH\")"
      ],
      "metadata": {
        "id": "maqyhzYA38Uq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "thDT7uQapINe",
        "outputId": "12d3f7ae-4f40-4218-e0a7-f98dc49b2b74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WHAT IMAGE YOU WANT TO RUN VISUALIZATIONS ON\n"
          ]
        }
      ],
      "source": [
        "print(TENPERCENT_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up imports"
      ],
      "metadata": {
        "id": "qIiAd2Mu4v6B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9au2dY-5bG--"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import models, transforms, utils\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.misc\n",
        "from PIL import Image\n",
        "import json\n",
        "import cv2\n",
        "import sys\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up model to load Eln-AI-ny back in"
      ],
      "metadata": {
        "id": "od71alfd4xbN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdVMpTZsYcfv"
      },
      "outputs": [],
      "source": [
        "class Global_Model(nn.Module): # model to combina both tab and imag \n",
        "    def __init__(self , pretrained_medical_resnet):\n",
        "        super().__init__()\n",
        "        self.conv_model = pretrained_medical_resnet # after freezing some layers and setting regression node as 1 not 4 \n",
        "        self.tab_model = nn.Sequential(nn.Linear(13, 500),\n",
        "                                  nn.BatchNorm1d(500),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Dropout(p=0.2),\n",
        "                                  nn.Linear(500,250),\n",
        "                                  nn.BatchNorm1d(250),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Dropout(p=0.2))\n",
        "        self.output = nn.Linear(256 + 250 , 1 , bias = True) # addition of output of concat for image  and tab then have final output \n",
        "    \n",
        "    def forward(self, x,tab): # x image , tab is tabular data related \n",
        "        x = self.conv_model(x)\n",
        "        tab_out = self.tab_model(tab)\n",
        "        x = torch.cat([x, tab_out],dim=1)\n",
        "        return self.output(x)\n",
        "\n",
        "    def saveConvParams(self): \n",
        "        torch.save(self.conv_model.state_dict(), f'')\n",
        "\n",
        "    def getConv(self): \n",
        "            return self.conv_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Redownload 10Percent\n",
        "\n",
        "Credit: https://github.com/ozanciga/self-supervised-histopathology/tree/tenpercent\n",
        "\n",
        "Paper: https://arxiv.org/pdf/2011.13971.pdf"
      ],
      "metadata": {
        "id": "MIq74xQ541rv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wwmb9hboYqp4"
      },
      "outputs": [],
      "source": [
        "RETURN_PREACTIVATION = False  # return features from the model, if false return classification logits\n",
        "NUM_CLASSES = 1  # only used if RETURN_PREACTIVATION = False\n",
        "\n",
        "\n",
        "def load_model_weights(model, weights):\n",
        "\n",
        "    model_dict = model.state_dict()\n",
        "    weights = {k: v for k, v in weights.items() if k in model_dict}\n",
        "    if weights == {}:\n",
        "        print('No weight could be loaded..')\n",
        "    model_dict.update(weights)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = torchvision.models.__dict__['resnet18'](pretrained=False)\n",
        "state = torch.load(TENPERCENT_MODEL_PATH, map_location='cuda:0')\n",
        "\n",
        "state_dict = state['state_dict']\n",
        "for key in list(state_dict.keys()):\n",
        "    state_dict[key.replace('model.', '').replace('resnet.', '')] = state_dict.pop(key)\n",
        "\n",
        "model = load_model_weights(model, state_dict)\n",
        "\n",
        "if RETURN_PREACTIVATION:\n",
        "    model.fc = torch.nn.Sequential()\n",
        "else:\n",
        "    model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "\n",
        "model = model.cuda()\n",
        "\n",
        "images = torch.rand((10, 3, 224, 224), device='cuda')\n",
        "\n",
        "out = model(images)\n",
        "\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "    # Replace the last fully-connected layer\n",
        "    # Parameters of newly constructed modules have requires_grad=True by default\n",
        "    \n",
        "    model.fc = torch.nn.Linear(512, 256,bias = True) \n",
        "    model = model.cuda()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load your model weights"
      ],
      "metadata": {
        "id": "3VlmalbU46IK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL3Ty_PEYgLd",
        "outputId": "04674b8a-a089-439d-9a1a-49b8ac589645"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['output.weight', 'output.bias'], unexpected_keys=['output.0.weight', 'output.0.bias', 'output.3.weight', 'output.3.bias', 'output.6.weight', 'output.6.bias', 'output.9.weight', 'output.9.bias', 'output.12.weight', 'output.12.bias'])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "proposed_model = copy.deepcopy(model)\n",
        "\n",
        "proposed_model = proposed_model.cuda()\n",
        "\n",
        "model_device = torch.device('cpu')\n",
        "\n",
        "model = copy.deepcopy(Global_Model(copy.deepcopy(proposed_model))) \n",
        "\n",
        "model.load_state_dict(torch.load(str(MODEL_PATH) , map_location = model_device), strict=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up visualization functions \n",
        "Credit and forked to work with Regressors from: https://github.com/jacobgil/pytorch-explain-black-box\n",
        "\n",
        "Original paper: https://arxiv.org/abs/1704.03296"
      ],
      "metadata": {
        "id": "WxXgc_ix5RBq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBJfc1-FZQWx"
      },
      "outputs": [],
      "source": [
        "use_cuda = True\n",
        "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
        "Tensor = FloatTensor\n",
        "\n",
        "def tv_norm(input, tv_beta):\n",
        "\timg = input[0, 0, :]\n",
        "\trow_grad = torch.mean(torch.abs((img[:-1 , :] - img[1 :, :])).pow(tv_beta))\n",
        "\tcol_grad = torch.mean(torch.abs((img[: , :-1] - img[: , 1 :])).pow(tv_beta))\n",
        "\treturn row_grad + col_grad\n",
        "\n",
        "def preprocess_image(img):\n",
        "\tmeans=[0.485, 0.456, 0.406]\n",
        "\tstds=[0.229, 0.224, 0.225]\n",
        "\n",
        "\tpreprocessed_img = img.copy()[: , :, ::-1]\n",
        "\tfor i in range(3):\n",
        "\t\tpreprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n",
        "\t\tpreprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n",
        "\tpreprocessed_img = \\\n",
        "\t\tnp.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))\n",
        "\n",
        "\tif use_cuda:\n",
        "\t\tpreprocessed_img_tensor = torch.from_numpy(preprocessed_img).cuda()\n",
        "\telse:\n",
        "\t\tpreprocessed_img_tensor = torch.from_numpy(preprocessed_img)\n",
        "\n",
        "\tpreprocessed_img_tensor.unsqueeze_(0)\n",
        "\treturn Variable(preprocessed_img_tensor, requires_grad = False)\n",
        "\n",
        "def save(mask, img, blurred, img_name, folder):\n",
        "\tmask = mask.cpu().data.numpy()[0]\n",
        "\tmask = np.transpose(mask, (1, 2, 0))\n",
        "\n",
        "\tmask = (mask - np.min(mask)) / np.max(mask)\n",
        "\tmask = 1 - mask\n",
        "\theatmap = cv2.applyColorMap(np.uint8(255*mask), cv2.COLORMAP_JET)\n",
        "\t\n",
        "\theatmap = np.float32(heatmap) / 255\n",
        "\tcam = 1.0*heatmap + np.float32(img)/255\n",
        "\tcam = cam / np.max(cam)\n",
        "\n",
        "\timg = np.float32(img) / 255\n",
        "\tperturbated = np.multiply(1 - mask, img) + np.multiply(mask, blurred)\t\n",
        " \n",
        "\tcv2.imwrite(folder + img_name.split('/')[-1] + \"_perturbated.png\", np.uint8(255*perturbated))\n",
        "\tcv2.imwrite(folder + img_name.split('/')[-1] + \"_heatmap.png\", np.uint8(255*heatmap))\n",
        "\tcv2.imwrite(folder + img_name.split('/')[-1] + \"_mask.png\", np.uint8(255*mask))\n",
        "\tcv2.imwrite(folder + img_name.split('/')[-1] + \"_cam.png\", np.uint8(255*cam))\n",
        "\n",
        "def numpy_to_torch(img, requires_grad = True):\n",
        "\tif len(img.shape) < 3:\n",
        "\t\toutput = np.float32([img])\n",
        "\telse:\n",
        "\t\toutput = np.transpose(img, (2, 0, 1))\n",
        "\n",
        "\toutput = torch.from_numpy(output)\n",
        "\tif use_cuda:\n",
        "\t\toutput = output.cuda()\n",
        "\n",
        "\toutput.unsqueeze_(0)\n",
        "\tv = Variable(output, requires_grad = requires_grad)\n",
        "\treturn v\n",
        "\n",
        "def genGradCam(img_name, model, folder):\n",
        "  tv_beta = 3\n",
        "  learning_rate = 0.1\n",
        "  max_iterations = 500\n",
        "  l1_coeff = 0.01\n",
        "  tv_coeff = 0.2\n",
        "\n",
        "  cnn_model = model.conv_model\n",
        "  original_img = cv2.imread(img_name)\n",
        "  original_img = cv2.resize(original_img, (224, 224))\n",
        "  img = np.float32(original_img) / 255\n",
        "  blurred_img1 = cv2.GaussianBlur(img, (11, 11), 5)\n",
        "  blurred_img2 = np.float32(cv2.medianBlur(original_img, 11))/255\n",
        "  blurred_img_numpy = (blurred_img1 + blurred_img2) / 2\n",
        "  mask_init = np.ones((28, 28), dtype = np.float32)\n",
        "\n",
        "  # Convert to torch variables\n",
        "  img = preprocess_image(img)\n",
        "  blurred_img = preprocess_image(blurred_img2)\n",
        "  mask = numpy_to_torch(mask_init)\n",
        "\n",
        "  if use_cuda:\n",
        "    upsample = torch.nn.UpsamplingBilinear2d(size=(224, 224)).cuda()\n",
        "  else:\n",
        "    upsample = torch.nn.UpsamplingBilinear2d(size=(224, 224))\n",
        "  optimizer = torch.optim.Adam([mask], lr=learning_rate)\n",
        "\n",
        "  target = torch.nn.ReLU()(cnn_model(img))\n",
        "  category = np.argmax(target.cpu().data.numpy())\n",
        "  print(\"Category with highest probability\", category)\n",
        "  print( \"Optimizing.. \")\n",
        "\n",
        "  for i in range(max_iterations):\n",
        "    upsampled_mask = upsample(mask)\n",
        "    # The single channel mask is used with an RGB image, \n",
        "    # so the mask is duplicated to have 3 channel,\n",
        "    upsampled_mask = \\\n",
        "      upsampled_mask.expand(1, 3, upsampled_mask.size(2), \\\n",
        "                    upsampled_mask.size(3))\n",
        "    \n",
        "    # Use the mask to perturbated the input image.\n",
        "    perturbated_input = img.mul(upsampled_mask) + \\\n",
        "              blurred_img.mul(1-upsampled_mask)\n",
        "    \n",
        "    noise = np.zeros((224, 224, 3), dtype = np.float32)\n",
        "    cv2.randn(noise, 0, 0.2)\n",
        "    noise = numpy_to_torch(noise)\n",
        "    perturbated_input = perturbated_input + noise\n",
        "    \n",
        "    outputs = torch.nn.ReLU()(cnn_model(perturbated_input))\n",
        "    loss = l1_coeff*torch.mean(torch.abs(1 - mask)) + \\\n",
        "        tv_coeff*tv_norm(mask, tv_beta) + outputs[0, category]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Optional: clamping seems to give better results\n",
        "    mask.data.clamp_(0, 1)\n",
        "\n",
        "  upsampled_mask = upsample(mask)\n",
        "  save(upsampled_mask, original_img, blurred_img_numpy, img_name, folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run and output CAMS"
      ],
      "metadata": {
        "id": "k09RzEkx5jDl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "5YsJGCnjbEUB",
        "outputId": "e9418000-55b5-4d21-968e-a601b01c3bbe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimg_name = \\'/content/drive/MyDrive/Epidemium Season 3/ORLIAn /Analysis/8aab52_[7778,34531]_component_data.tif.jpg\\'\\n#img_name = \\'/content/drive/MyDrive/Epidemium Season 3/ORLIAn /Analysis/8b44cd_[13491,59805]_component_data.tif.jpg\\'\\ngenGradCam(img_name, model, \"/content/drive/MyDrive/Epidemium Season 3/ORLIAn /Outputs/OtherModels/\")\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img_name = IMG_TO_ANALYZE\n",
        "genGradCam(img_name, model, FOLDER_TO_OUTPUT)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Generate_Visualizations_Using_GradCam.ipynb",
      "provenance": [],
      "mount_file_id": "19raNlMLwP5q8T8fBL6r8Y8h3j95jpOoM",
      "authorship_tag": "ABX9TyNy+JC3lIMhxShViGcejCBS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}